{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "sales = pd.read_csv(\"sales_data_set.csv\")\n",
        "stores = pd.read_csv(\"stores_data_set.csv\")\n",
        "features = pd.read_csv(\"Features_data_set.csv\")\n",
        "\n",
        "sales['Date'] = pd.to_datetime(sales['Date'], dayfirst=True)\n",
        "features['Date'] = pd.to_datetime(features['Date'], dayfirst=True)\n",
        "\n",
        "df = (\n",
        "    sales\n",
        "    .merge(stores, on=\"Store\", how=\"left\")\n",
        "    .merge(features, on=[\"Store\", \"Date\", \"IsHoliday\"], how=\"left\")\n",
        ")\n",
        "\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "```\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Year-over-Year (YoY) growth per Store–Dept\n",
        "\n",
        "**Q1.** For each `(Store, Dept, Year)`, compute total `Weekly_Sales` and then calculate **year-over-year percentage growth** of that department in that store.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "sales_y = (\n",
        "    df.groupby(['Store', 'Dept', 'Year'], as_index=False)['Weekly_Sales']\n",
        "      .sum()\n",
        "      .sort_values(['Store', 'Dept', 'Year'])\n",
        ")\n",
        "\n",
        "sales_y['YoY_Growth_%'] = (\n",
        "    sales_y\n",
        "    .groupby(['Store', 'Dept'])['Weekly_Sales']\n",
        "    .pct_change() * 100\n",
        ")\n",
        "\n",
        "sales_y.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Rank stores within each type, per year\n",
        "\n",
        "**Q2.** For each `Year` and `Type`, rank stores by total `Weekly_Sales` (1 = highest) and keep only the **top 3** stores per type per year.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "year_type_store = (\n",
        "    df.groupby(['Year', 'Type', 'Store'], as_index=False)['Weekly_Sales']\n",
        "      .sum()\n",
        ")\n",
        "\n",
        "year_type_store['Rank'] = (\n",
        "    year_type_store\n",
        "    .groupby(['Year', 'Type'])['Weekly_Sales']\n",
        "    .rank(method='dense', ascending=False)\n",
        ")\n",
        "\n",
        "top3_per_type_year = year_type_store[year_type_store['Rank'] <= 3]\n",
        "top3_per_type_year.sort_values(['Year', 'Type', 'Rank'])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Normalize sales within each Store–Year\n",
        "\n",
        "**Q3.** Within each `(Store, Year)`, normalize `Weekly_Sales` to have mean 0 and std 1 (z-score).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "def zscore(x):\n",
        "    return (x - x.mean()) / x.std()\n",
        "\n",
        "df = df.sort_values(['Store', 'Year', 'Date'])\n",
        "df['StoreYear_Z_Sales'] = (\n",
        "    df.groupby(['Store', 'Year'])['Weekly_Sales'].transform(zscore)\n",
        ")\n",
        "df[['Store', 'Year', 'Weekly_Sales', 'StoreYear_Z_Sales']].head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Detect outliers per Dept using IQR\n",
        "\n",
        "**Q4.** For each `Dept`, flag rows where `Weekly_Sales` is an **outlier** using the IQR rule (outside [Q1 − 1.5·IQR, Q3 + 1.5·IQR]).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "def iqr_outlier_flags(s):\n",
        "    q1 = s.quantile(0.25)\n",
        "    q3 = s.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower = q1 - 1.5 * iqr\n",
        "    upper = q3 + 1.5 * iqr\n",
        "    return (s < lower) | (s > upper)\n",
        "\n",
        "df['Dept_Outlier'] = (\n",
        "    df.groupby('Dept')['Weekly_Sales']\n",
        "      .transform(iqr_outlier_flags)\n",
        ")\n",
        "\n",
        "df[df['Dept_Outlier']].head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Weekly chain sales with moving maximum drawdown\n",
        "\n",
        "**Q5.** Aggregate chain-level weekly sales (by `Date`), then compute running **cumulative max** and the **drawdown** (percentage drop from the peak).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "weekly_chain = (\n",
        "    df.groupby('Date', as_index=False)['Weekly_Sales']\n",
        "      .sum()\n",
        "      .sort_values('Date')\n",
        ")\n",
        "\n",
        "weekly_chain['CumMax'] = weekly_chain['Weekly_Sales'].cummax()\n",
        "weekly_chain['Drawdown_%'] = (\n",
        "    (weekly_chain['Weekly_Sales'] - weekly_chain['CumMax'])\n",
        "    / weekly_chain['CumMax'] * 100\n",
        ")\n",
        "\n",
        "weekly_chain.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Rolling 8-week mean and std per Store\n",
        "\n",
        "**Q6.** For each store, compute an 8-week rolling **mean** and **std** of `Weekly_Sales` (sorted by `Date`).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df = df.sort_values(['Store', 'Date'])\n",
        "\n",
        "df['Roll8_Mean'] = (\n",
        "    df.groupby('Store')['Weekly_Sales']\n",
        "      .transform(lambda s: s.rolling(8, min_periods=4).mean())\n",
        ")\n",
        "\n",
        "df['Roll8_Std'] = (\n",
        "    df.groupby('Store')['Weekly_Sales']\n",
        "      .transform(lambda s: s.rolling(8, min_periods=4).std())\n",
        ")\n",
        "\n",
        "df[['Store', 'Date', 'Weekly_Sales', 'Roll8_Mean', 'Roll8_Std']].head(15)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Rolling correlation between Temperature & Weekly_Sales\n",
        "\n",
        "**Q7.** For each store, compute a 12-week rolling **correlation** between `Temperature` and `Weekly_Sales`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "def rolling_corr(g):\n",
        "    return (\n",
        "        g[['Weekly_Sales', 'Temperature']]\n",
        "        .rolling(12, min_periods=6)\n",
        "        .corr()\n",
        "        .reset_index(level=0, drop=True)\n",
        "        .loc[:, ('Weekly_Sales', 'Temperature')]\n",
        "    )\n",
        "\n",
        "df = df.sort_values(['Store', 'Date'])\n",
        "df['Roll12_Corr_Temp_Sales'] = (\n",
        "    df.groupby('Store', group_keys=False).apply(rolling_corr)\n",
        ")\n",
        "\n",
        "df[['Store', 'Date', 'Roll12_Corr_Temp_Sales']].head(20)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Monthly sales and Month-over-Month (MoM) growth\n",
        "\n",
        "**Q8.** Compute **monthly** chain-level total sales and then Month-over-Month growth (`pct_change`).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "monthly_chain = (\n",
        "    df.set_index('Date')\n",
        "      .resample('M')['Weekly_Sales']\n",
        "      .sum()\n",
        "      .to_frame('Monthly_Sales')\n",
        ")\n",
        "\n",
        "monthly_chain['MoM_Growth_%'] = monthly_chain['Monthly_Sales'].pct_change() * 100\n",
        "monthly_chain.head(12)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Use `pd.Grouper` for store-level monthly sales\n",
        "\n",
        "**Q9.** Using `pd.Grouper`, compute monthly total `Weekly_Sales` per store (Date = month end).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "store_monthly = (\n",
        "    df.groupby(['Store', pd.Grouper(key='Date', freq='M')])['Weekly_Sales']\n",
        "      .sum()\n",
        "      .reset_index()\n",
        "      .rename(columns={'Weekly_Sales': 'Monthly_Sales'})\n",
        ")\n",
        "\n",
        "store_monthly.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Find the “most seasonal” departments (highest variance across weeks)\n",
        "\n",
        "**Q10.** Aggregate chain-level weekly sales per `Dept` and `Date`. For each dept, compute the **variance** over time and list top 5 depts with highest variance (most volatile).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "dept_week = (\n",
        "    df.groupby(['Dept', 'Date'], as_index=False)['Weekly_Sales']\n",
        "      .sum()\n",
        ")\n",
        "\n",
        "dept_var = (\n",
        "    dept_week.groupby('Dept')['Weekly_Sales']\n",
        "             .var()\n",
        "             .sort_values(ascending=False)\n",
        ")\n",
        "\n",
        "dept_var.head(5)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Store–Dept combination with highest sales on holidays vs non-holidays\n",
        "\n",
        "**Q11.** For each `(Store, Dept)` compute total sales on **holidays** and **non-holidays**; then find the combination where the **holiday share** is highest.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "agg = (\n",
        "    df.groupby(['Store', 'Dept', 'IsHoliday'])['Weekly_Sales']\n",
        "      .sum()\n",
        "      .unstack('IsHoliday', fill_value=0)\n",
        "      .rename(columns={False: 'NonHoliday', True: 'Holiday'})\n",
        ")\n",
        "\n",
        "agg['Holiday_Share'] = agg['Holiday'] / (agg['Holiday'] + agg['NonHoliday'])\n",
        "agg['Holiday_Share'].sort_values(ascending=False).head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Complex pivot: Type × Year with multiple aggregations\n",
        "\n",
        "**Q12.** Create a pivot table with index = `Type`, columns = `Year`, and values = `Weekly_Sales`, showing both **mean** and **sum**, with margins.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "pivot_type_year = pd.pivot_table(\n",
        "    df,\n",
        "    index='Type',\n",
        "    columns='Year',\n",
        "    values='Weekly_Sales',\n",
        "    aggfunc=['mean', 'sum'],\n",
        "    margins=True\n",
        ")\n",
        "\n",
        "pivot_type_year\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 13. Melt MarkDown columns for long-format analysis\n",
        "\n",
        "**Q13.** Convert the 5 `MarkDown` columns into a **long** format with columns: `Store, Date, Dept, MarkDown_Type, MarkDown_Value`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "\n",
        "md_long = df.melt(\n",
        "    id_vars=['Store', 'Dept', 'Date'],\n",
        "    value_vars=markdown_cols,\n",
        "    var_name='MarkDown_Type',\n",
        "    value_name='MarkDown_Value'\n",
        ")\n",
        "\n",
        "md_long.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 14. Effect of MarkDown on sales: correlation per department\n",
        "\n",
        "**Q14.** Using `md_long`, compute the correlation between `MarkDown_Value` and `Weekly_Sales` for each `Dept`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "# Merge md_long back with Weekly_Sales\n",
        "md_merged = md_long.merge(\n",
        "    df[['Store', 'Dept', 'Date', 'Weekly_Sales']],\n",
        "    on=['Store', 'Dept', 'Date'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "dept_md_corr = (\n",
        "    md_merged.groupby('Dept')\n",
        "             .apply(lambda g: g['Weekly_Sales'].corr(g['MarkDown_Value']))\n",
        "             .rename('Corr_MarkDown_Sales')\n",
        ")\n",
        "\n",
        "dept_md_corr.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 15. Top date of maximum sales per Dept (with Store info)\n",
        "\n",
        "**Q15.** For each `Dept`, find the row with **maximum** `Weekly_Sales` (store and date) and return a tidy DataFrame with `Dept, Store, Date, Weekly_Sales`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "idx_max = df.groupby('Dept')['Weekly_Sales'].idxmax()\n",
        "dept_max_rows = df.loc[idx_max, ['Dept', 'Store', 'Date', 'Weekly_Sales']]\n",
        "dept_max_rows.sort_values('Dept')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 16. Correlation matrix of numeric columns; top correlations with Weekly_Sales\n",
        "\n",
        "**Q16.** Compute the correlation matrix of **all numeric** columns and list the **top 5 features** most strongly correlated (absolute) with `Weekly_Sales`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "num_df = df.select_dtypes(include='number')\n",
        "corr = num_df.corr()\n",
        "\n",
        "target_corr = corr['Weekly_Sales'].drop('Weekly_Sales')\n",
        "top5 = target_corr.reindex(target_corr.abs().sort_values(ascending=False).index).head(5)\n",
        "top5\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 17. Segment stores into performance tiers based on total sales\n",
        "\n",
        "**Q17.** Compute total sales per store and use `pd.qcut` to assign each store into 3 **performance tiers**: `Low`, `Medium`, `High`. Add this as `Performance_Tier` into `df`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "store_totals = df.groupby('Store')['Weekly_Sales'].sum()\n",
        "\n",
        "tiers = pd.qcut(store_totals, q=3, labels=['Low', 'Medium', 'High'])\n",
        "store_tier = tiers.to_frame('Performance_Tier').reset_index()\n",
        "\n",
        "df = df.merge(store_tier, on='Store', how='left')\n",
        "df[['Store', 'Performance_Tier']].drop_duplicates().head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 18. Compare holiday vs non-holiday sales per performance tier\n",
        "\n",
        "**Q18.** Using `Performance_Tier`, compute average `Weekly_Sales` on holidays and non-holidays for each tier.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "tier_holiday_sales = (\n",
        "    df.groupby(['Performance_Tier', 'IsHoliday'])['Weekly_Sales']\n",
        "      .mean()\n",
        "      .unstack('IsHoliday')\n",
        "      .rename(columns={False: 'NonHoliday_Mean', True: 'Holiday_Mean'})\n",
        ")\n",
        "\n",
        "tier_holiday_sales\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 19. Build a custom summary function and apply per store\n",
        "\n",
        "**Q19.** Define a function that returns a Series with: `total_sales`, `mean_sales`, `std_sales`, `num_weeks`, `pct_holiday_weeks`. Apply this function per `Store`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "def store_summary(g):\n",
        "    return pd.Series({\n",
        "        'total_sales': g['Weekly_Sales'].sum(),\n",
        "        'mean_sales': g['Weekly_Sales'].mean(),\n",
        "        'std_sales': g['Weekly_Sales'].std(),\n",
        "        'num_weeks': g['Date'].nunique(),\n",
        "        'pct_holiday_weeks': g['IsHoliday'].mean() * 100\n",
        "    })\n",
        "\n",
        "store_summary_df = df.groupby('Store').apply(store_summary).reset_index()\n",
        "store_summary_df.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 20. MultiIndex time series: store × dept weekly panel\n",
        "\n",
        "**Q20.** Build a panel where index is `(Store, Dept, Date)` and column is `Weekly_Sales`, then fill missing combinations with 0.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "panel = (\n",
        "    df.set_index(['Store', 'Dept', 'Date'])['Weekly_Sales']\n",
        ")\n",
        "\n",
        "panel_full = (\n",
        "    panel.unstack(level=['Store', 'Dept'])  # wide in Store–Dept\n",
        "         .fillna(0)\n",
        "         .stack(['Store', 'Dept'])          # back to long but with 0 for missing\n",
        "         .to_frame('Weekly_Sales')\n",
        ")\n",
        "\n",
        "panel_full.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 21. Calculate lagged sales 1 week and 52 weeks ago per Store–Dept\n",
        "\n",
        "**Q21.** For each `(Store, Dept)`, compute `Lag_1_Week` and `Lag_52_Week` sales (shifted by 1 and 52 rows, assuming weekly data with regular frequency).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df = df.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "group = df.groupby(['Store', 'Dept'])['Weekly_Sales']\n",
        "df['Lag_1_Week'] = group.shift(1)\n",
        "df['Lag_52_Week'] = group.shift(52)\n",
        "\n",
        "df[['Store', 'Dept', 'Date', 'Weekly_Sales', 'Lag_1_Week', 'Lag_52_Week']].head(20)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 22. CPI quantile segments and sales\n",
        "\n",
        "**Q22.** Bin `CPI` into quartiles using `pd.qcut`, label them `Q1`–`Q4`, and compute mean `Weekly_Sales` in each CPI segment.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df['CPI_Quartile'] = pd.qcut(df['CPI'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
        "\n",
        "cpi_sales = df.groupby('CPI_Quartile')['Weekly_Sales'].mean()\n",
        "cpi_sales\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 23. Unemployment–CPI matrix of mean sales\n",
        "\n",
        "**Q23.** Discretize both `Unemployment` and `CPI` into 4 bins each and create a 4×4 matrix of mean `Weekly_Sales` for each combination.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df['Unemp_Bin'] = pd.qcut(df['Unemployment'], q=4, labels=False)\n",
        "df['CPI_Bin'] = pd.qcut(df['CPI'], q=4, labels=False)\n",
        "\n",
        "unemp_cpi_matrix = (\n",
        "    df.pivot_table(\n",
        "        index='Unemp_Bin',\n",
        "        columns='CPI_Bin',\n",
        "        values='Weekly_Sales',\n",
        "        aggfunc='mean'\n",
        "    )\n",
        ")\n",
        "\n",
        "unemp_cpi_matrix\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 24. Highest increase week per Store–Dept\n",
        "\n",
        "**Q24.** For each `(Store, Dept)`, find the week with the **largest positive week-over-week jump** in `Weekly_Sales`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df = df.sort_values(['Store', 'Dept', 'Date'])\n",
        "df['Prev_Sales'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1)\n",
        "df['WoW_Change'] = df['Weekly_Sales'] - df['Prev_Sales']\n",
        "\n",
        "idx_max_jump = (\n",
        "    df.groupby(['Store', 'Dept'])['WoW_Change']\n",
        "      .idxmax()\n",
        "      .dropna()\n",
        "      .astype(int)\n",
        ")\n",
        "\n",
        "max_jump_rows = df.loc[idx_max_jump,\n",
        "                       ['Store', 'Dept', 'Date', 'Weekly_Sales', 'WoW_Change']]\n",
        "max_jump_rows.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 25. Share of each Dept in a store’s annual sales\n",
        "\n",
        "**Q25.** For each `(Store, Year, Dept)`, compute total sales and then calculate **department share** (dept_sales / store_year_sales).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "store_year_dept = (\n",
        "    df.groupby(['Store', 'Year', 'Dept'], as_index=False)['Weekly_Sales']\n",
        "      .sum()\n",
        "      .rename(columns={'Weekly_Sales': 'Dept_Sales'})\n",
        ")\n",
        "\n",
        "store_year = (\n",
        "    store_year_dept.groupby(['Store', 'Year'])['Dept_Sales']\n",
        "                   .sum()\n",
        "                   .rename('StoreYear_Sales')\n",
        "                   .reset_index()\n",
        ")\n",
        "\n",
        "store_year_dept = store_year_dept.merge(store_year, on=['Store', 'Year'], how='left')\n",
        "store_year_dept['Dept_Share'] = store_year_dept['Dept_Sales'] / store_year_dept['StoreYear_Sales']\n",
        "\n",
        "store_year_dept.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 26. Pivot: Store × Year with Dept share concentration (Herfindahl index)\n",
        "\n",
        "**Q26.** Using `store_year_dept` from Q25, compute a **Herfindahl-like index** of dept concentration per `(Store, Year)` = sum of (Dept_Share²).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "herfindahl = (\n",
        "    store_year_dept\n",
        "    .assign(Share_Sq=lambda x: x['Dept_Share'] ** 2)\n",
        "    .groupby(['Store', 'Year'])['Share_Sq']\n",
        "    .sum()\n",
        "    .rename('Dept_Herfindahl')\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "herfindahl.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 27. Compare actual sales vs rolling median\n",
        "\n",
        "**Q27.** For each `(Store, Dept)`, compute a 9-week rolling **median** of `Weekly_Sales` and create a column `Above_Median` (True if current > rolling median).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df = df.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "rolling_median = (\n",
        "    df.groupby(['Store', 'Dept'])['Weekly_Sales']\n",
        "      .transform(lambda s: s.rolling(9, min_periods=5).median())\n",
        ")\n",
        "\n",
        "df['Roll9_Median'] = rolling_median\n",
        "df['Above_Median'] = df['Weekly_Sales'] > df['Roll9_Median']\n",
        "\n",
        "df[['Store', 'Dept', 'Date', 'Weekly_Sales', 'Roll9_Median', 'Above_Median']].head(20)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 28. Identify “hot” weeks (unusually high sales)\n",
        "\n",
        "**Q28.** Mark a week as “hot” (`Is_Hot_Week`) if its `Weekly_Sales` is **more than 2 standard deviations above** the store’s mean.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "store_stats = (\n",
        "    df.groupby('Store')['Weekly_Sales']\n",
        "      .agg(['mean', 'std'])\n",
        "      .rename(columns={'mean': 'Store_Mean', 'std': 'Store_Std'})\n",
        ")\n",
        "\n",
        "df = df.merge(store_stats, left_on='Store', right_index=True, how='left')\n",
        "\n",
        "df['Is_Hot_Week'] = df['Weekly_Sales'] > (df['Store_Mean'] + 2 * df['Store_Std'])\n",
        "\n",
        "df[['Store', 'Date', 'Weekly_Sales', 'Is_Hot_Week']].head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 29. For each Year, find which Store had the highest average temperature and its mean sales\n",
        "\n",
        "**Q29.** For every `Year`, find the **store** with the highest average `Temperature` and report that store along with its mean `Weekly_Sales` in that year.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "year_store_temp = (\n",
        "    df.groupby(['Year', 'Store'], as_index=False)\n",
        "      .agg(Avg_Temp=('Temperature', 'mean'),\n",
        "           Avg_Sales=('Weekly_Sales', 'mean'))\n",
        ")\n",
        "\n",
        "idx = year_store_temp.groupby('Year')['Avg_Temp'].idxmax()\n",
        "hottest_stores = year_store_temp.loc[idx].sort_values('Year')\n",
        "hottest_stores\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 30. Complex chained operation: holiday boost factor per Dept\n",
        "\n",
        "**Q30.** For each department, compute:\n",
        "\n",
        "* mean sales on holidays (`Holiday_Mean`)\n",
        "* mean sales on non-holidays (`NonHoliday_Mean`)\n",
        "* **Holiday Boost Factor** = `Holiday_Mean / NonHoliday_Mean`\n",
        "\n",
        "Sort departments by boost factor descending.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "dept_holiday = (\n",
        "    df.groupby(['Dept', 'IsHoliday'])['Weekly_Sales']\n",
        "      .mean()\n",
        "      .unstack('IsHoliday')\n",
        "      .rename(columns={False: 'NonHoliday_Mean', True: 'Holiday_Mean'})\n",
        ")\n",
        "\n",
        "dept_holiday['Holiday_Boost_Factor'] = (\n",
        "    dept_holiday['Holiday_Mean'] / dept_holiday['NonHoliday_Mean']\n",
        ")\n",
        "\n",
        "dept_holiday.sort_values('Holiday_Boost_Factor', ascending=False).head(10)\n",
        "```\n",
        "\n",
        "\n",
        "## 31. Cumulative market share by store (chain-level)\n",
        "\n",
        "**Q31.** Compute total `Weekly_Sales` per store, then calculate each store’s **market share** (%) of total chain sales and **cumulative** share sorted descending by sales.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "store_totals = df.groupby('Store')['Weekly_Sales'].sum().sort_values(ascending=False)\n",
        "total_chain = store_totals.sum()\n",
        "\n",
        "market_share = (store_totals / total_chain * 100).to_frame('Market_Share_%')\n",
        "market_share['Cum_Market_Share_%'] = market_share['Market_Share_%'].cumsum()\n",
        "market_share.head(10)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 32. Top 3 departments per store by total sales\n",
        "\n",
        "**Q32.** For each store, find the **top 3 departments** by total `Weekly_Sales`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "store_dept_totals = (\n",
        "    df.groupby(['Store', 'Dept'])['Weekly_Sales']\n",
        "      .sum()\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "store_dept_totals['Rank'] = (\n",
        "    store_dept_totals.groupby('Store')['Weekly_Sales']\n",
        "                     .rank(method='dense', ascending=False)\n",
        ")\n",
        "\n",
        "top3_depts_per_store = store_dept_totals[store_dept_totals['Rank'] <= 3] \\\n",
        "    .sort_values(['Store', 'Rank'])\n",
        "top3_depts_per_store.head(20)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 33. Department concentration index per store (Herfindahl at store level)\n",
        "\n",
        "**Q33.** For each store, compute dept share in that store’s total sales and the **Herfindahl index** = sum(Dept_Share²).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "sd = (\n",
        "    df.groupby(['Store', 'Dept'])['Weekly_Sales']\n",
        "      .sum()\n",
        "      .rename('Dept_Sales')\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "store_tot = sd.groupby('Store')['Dept_Sales'].sum().rename('Store_Sales')\n",
        "sd = sd.merge(store_tot, on='Store', how='left')\n",
        "sd['Dept_Share'] = sd['Dept_Sales'] / sd['Store_Sales']\n",
        "\n",
        "herfindahl_store = (\n",
        "    sd.assign(Share_Sq=lambda x: x['Dept_Share'] ** 2)\n",
        "      .groupby('Store')['Share_Sq']\n",
        "      .sum()\n",
        "      .rename('Dept_Herfindahl')\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "herfindahl_store.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 34. Compare sales distributions across store types\n",
        "\n",
        "**Q34.** For each store type, compute `Weekly_Sales` **median**, 25th and 75th percentile.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "type_quantiles = (\n",
        "    df.groupby('Type')['Weekly_Sales']\n",
        "      .quantile([0.25, 0.5, 0.75])\n",
        "      .unstack()\n",
        "      .rename(columns={0.25: 'Q1', 0.5: 'Median', 0.75: 'Q3'})\n",
        ")\n",
        "\n",
        "type_quantiles\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 35. Weekly sales per store on hottest week\n",
        "\n",
        "**Q35.** For every store, find the **hottest week** (max `Temperature`) and show that week’s `Weekly_Sales` and `Temperature`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "idx_hottest = df.groupby('Store')['Temperature'].idxmax()\n",
        "hottest_weeks = df.loc[idx_hottest, ['Store', 'Date', 'Temperature', 'Weekly_Sales']]\n",
        "hottest_weeks.sort_values('Store')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 36. Coldest vs hottest mean sales per store\n",
        "\n",
        "**Q36.** For each store, split rows into **top 10% hottest** and **bottom 10% coldest** (by temperature) and compare mean sales.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "def hot_cold_mean(g):\n",
        "    q_low = g['Temperature'].quantile(0.1)\n",
        "    q_high = g['Temperature'].quantile(0.9)\n",
        "    cold_mean = g[g['Temperature'] <= q_low]['Weekly_Sales'].mean()\n",
        "    hot_mean = g[g['Temperature'] >= q_high]['Weekly_Sales'].mean()\n",
        "    return pd.Series({'Cold_Mean_Sales': cold_mean, 'Hot_Mean_Sales': hot_mean})\n",
        "\n",
        "temp_effect = df.groupby('Store').apply(hot_cold_mean).reset_index()\n",
        "temp_effect.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 37. Identify stores with strong positive Temperature–Sales correlation\n",
        "\n",
        "**Q37.** Compute correlation between `Temperature` and `Weekly_Sales` per store and list stores with correlation > 0.3.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "temp_sales_corr = df.groupby('Store').apply(\n",
        "    lambda g: g['Weekly_Sales'].corr(g['Temperature'])\n",
        ").rename('Temp_Sales_Corr')\n",
        "\n",
        "temp_sales_corr[temp_sales_corr > 0.3].sort_values(ascending=False)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 38. Sales growth from first half to second half of each year\n",
        "\n",
        "**Q38.** For each `(Store, Year)`, compute total sales in **Jan–Jun** and **Jul–Dec**, and calculate growth % from first half to second half.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df['Half'] = df['Month'].apply(lambda m: 'H1' if m <= 6 else 'H2')\n",
        "\n",
        "syh = (\n",
        "    df.groupby(['Store', 'Year', 'Half'])['Weekly_Sales']\n",
        "      .sum()\n",
        "      .unstack('Half', fill_value=0)\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "syh['H2_vs_H1_Growth_%'] = (syh['H2'] - syh['H1']) / syh['H1'] * 100\n",
        "syh.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 39. Chain’s annual holiday uplift\n",
        "\n",
        "**Q39.** For each year, compute average sales on holiday weeks and non-holiday weeks and calculate **holiday uplift %**.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "yh = (\n",
        "    df.groupby(['Year', 'IsHoliday'])['Weekly_Sales']\n",
        "      .mean()\n",
        "      .unstack('IsHoliday')\n",
        "      .rename(columns={False: 'NonHoliday_Mean', True: 'Holiday_Mean'})\n",
        ")\n",
        "\n",
        "yh['Holiday_Uplift_%'] = (yh['Holiday_Mean'] - yh['NonHoliday_Mean']) \\\n",
        "                         / yh['NonHoliday_Mean'] * 100\n",
        "yh\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 40. Store “seasonality index” by quarter\n",
        "\n",
        "**Q40.** Create a `Quarter` column and compute, for each store, a **seasonality index** for each quarter = quarter mean / store annual mean.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df['Quarter'] = df['Date'].dt.to_period('Q').astype(str)\n",
        "\n",
        "store_q = df.groupby(['Store', 'Year', 'Quarter'])['Weekly_Sales'].mean().rename('Quarter_Mean')\n",
        "store_year_mean = df.groupby(['Store', 'Year'])['Weekly_Sales'].mean().rename('Store_Year_Mean')\n",
        "\n",
        "sidx = (\n",
        "    store_q.to_frame()\n",
        "           .join(store_year_mean, on=['Store', 'Year'])\n",
        "           .assign(Seasonality_Index=lambda x: x['Quarter_Mean'] / x['Store_Year_Mean'])\n",
        "           .reset_index()\n",
        ")\n",
        "\n",
        "sidx.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 41. Aggregate by ISO year-week using Grouper\n",
        "\n",
        "**Q41.** Build a table with index `(ISO_Year, ISO_Week)` and columns `Store`, showing total weekly sales per store (0 for missing).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "iso = df['Date'].dt.isocalendar()\n",
        "df['ISO_Year'] = iso.year\n",
        "df['ISO_Week'] = iso.week\n",
        "\n",
        "iso_store = (\n",
        "    df.groupby(['ISO_Year', 'ISO_Week', 'Store'])['Weekly_Sales']\n",
        "      .sum()\n",
        "      .unstack('Store', fill_value=0)\n",
        ")\n",
        "\n",
        "iso_store.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 42. Use `pipe` to make a clean transformation chain\n",
        "\n",
        "**Q42.** Using `.pipe`, create a function that returns **store-level annual summary** with total sales, avg temperature, and avg unemployment per year.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "def store_year_summary(data):\n",
        "    return (\n",
        "        data.groupby(['Store', 'Year'])\n",
        "            .agg(\n",
        "                Total_Sales=('Weekly_Sales', 'sum'),\n",
        "                Avg_Temp=('Temperature', 'mean'),\n",
        "                Avg_Unemp=('Unemployment', 'mean')\n",
        "            )\n",
        "            .reset_index()\n",
        "    )\n",
        "\n",
        "store_year_df = df.pipe(store_year_summary)\n",
        "store_year_df.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 43. Identify “data gaps” in weekly time series\n",
        "\n",
        "**Q43.** For a given `(Store, Dept)` (e.g., Store 1, Dept 1), check if there are any missing weekly dates between min and max date.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "sd = df[(df['Store'] == 1) & (df['Dept'] == 1)].sort_values('Date')\n",
        "\n",
        "all_weeks = pd.date_range(sd['Date'].min(), sd['Date'].max(), freq='W-FRI')\n",
        "missing_weeks = all_weeks.difference(sd['Date'].unique())\n",
        "\n",
        "missing_weeks\n",
        "```\n",
        "\n",
        "*(Change store/dept as needed.)*\n",
        "\n",
        "---\n",
        "\n",
        "## 44. Fill missing weeks with zero sales for a store-dept\n",
        "\n",
        "**Q44.** For `(Store=1, Dept=1)`, create a complete weekly series from min to max date and fill missing weeks with `Weekly_Sales = 0`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "sd = df[(df['Store'] == 1) & (df['Dept'] == 1)].copy()\n",
        "sd = sd.set_index('Date')\n",
        "\n",
        "full_idx = pd.date_range(sd.index.min(), sd.index.max(), freq='W-FRI')\n",
        "sd_full = (\n",
        "    sd.reindex(full_idx)\n",
        "      .rename_axis('Date')\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "sd_full['Store'] = sd_full['Store'].fillna(1)\n",
        "sd_full['Dept'] = sd_full['Dept'].fillna(1)\n",
        "sd_full['Weekly_Sales'] = sd_full['Weekly_Sales'].fillna(0)\n",
        "\n",
        "sd_full.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 45. Add a rank of weeks by sales within each Store–Dept\n",
        "\n",
        "**Q45.** For each `(Store, Dept)`, rank weeks by `Weekly_Sales` (1 = highest).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df = df.sort_values(['Store', 'Dept', 'Weekly_Sales'], ascending=[True, True, False])\n",
        "df['Sales_Rank_in_Dept'] = (\n",
        "    df.groupby(['Store', 'Dept'])['Weekly_Sales']\n",
        "      .rank(method='dense', ascending=False)\n",
        ")\n",
        "df[['Store', 'Dept', 'Date', 'Weekly_Sales', 'Sales_Rank_in_Dept']].head(20)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 46. Use `.where` and `.mask` to cap extreme sales\n",
        "\n",
        "**Q46.** Cap `Weekly_Sales` at the **99th percentile** chain-wide and store it in `Weekly_Sales_Capped`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "cap = df['Weekly_Sales'].quantile(0.99)\n",
        "df['Weekly_Sales_Capped'] = df['Weekly_Sales'].clip(upper=cap)\n",
        "df[['Weekly_Sales', 'Weekly_Sales_Capped']].head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 47. Holiday-adjusted sales (subtract annual average)\n",
        "\n",
        "**Q47.** For each year, subtract that year’s **mean sales** (chain-level) to get `Year_Detrended_Sales`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "year_mean = df.groupby('Year')['Weekly_Sales'].mean().rename('Year_Mean')\n",
        "df = df.merge(year_mean, on='Year', how='left')\n",
        "df['Year_Detrended_Sales'] = df['Weekly_Sales'] - df['Year_Mean']\n",
        "df[['Year', 'Weekly_Sales', 'Year_Mean', 'Year_Detrended_Sales']].head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 48. Build a wide table of Year vs Store with average sales\n",
        "\n",
        "**Q48.** Pivot `Year` as rows and `Store` as columns for **mean** `Weekly_Sales`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "year_store_mean = (\n",
        "    df.pivot_table(\n",
        "        index='Year',\n",
        "        columns='Store',\n",
        "        values='Weekly_Sales',\n",
        "        aggfunc='mean'\n",
        "    )\n",
        ")\n",
        "\n",
        "year_store_mean.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 49. Flatten a multi-level column index after pivoting\n",
        "\n",
        "**Q49.** Do a pivot with multiple agg functions (`sum` and `mean`) for `Weekly_Sales` by `Type` and `Year`, then flatten the column MultiIndex.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "pivot = pd.pivot_table(\n",
        "    df,\n",
        "    index='Type',\n",
        "    columns='Year',\n",
        "    values='Weekly_Sales',\n",
        "    aggfunc=['sum', 'mean']\n",
        ")\n",
        "\n",
        "pivot.columns = [f\"{agg}_{year}\" for agg, year in pivot.columns]\n",
        "pivot.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 50. Find dates where a store’s sales are above its 90th percentile\n",
        "\n",
        "**Q50.** For each store, mark `Is_Top10_Sales_Week` where `Weekly_Sales` > store’s 90th percentile.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "p90 = df.groupby('Store')['Weekly_Sales'].quantile(0.9).rename('P90')\n",
        "df = df.merge(p90, on='Store', how='left')\n",
        "df['Is_Top10_Sales_Week'] = df['Weekly_Sales'] > df['P90']\n",
        "df[['Store', 'Date', 'Weekly_Sales', 'P90', 'Is_Top10_Sales_Week']].head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 51. Use `groupby().nlargest()` to get top 2 weeks per store-type\n",
        "\n",
        "**Q51.** For each store `Type`, find the **top 2 weeks** (dates) by chain-level sales.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "type_date_sales = df.groupby(['Type', 'Date'])['Weekly_Sales'].sum()\n",
        "top2_per_type = type_date_sales.groupby('Type').nlargest(2).reset_index(name='Total_Sales')\n",
        "top2_per_type\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 52. Create a boolean “promo week” based on any MarkDown\n",
        "\n",
        "**Q52.** Define `Is_Promo_Week` as True if any MarkDown column > 0 for that row.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "df['Is_Promo_Week'] = (df[markdown_cols].fillna(0) > 0).any(axis=1)\n",
        "df[['Store', 'Date', 'Is_Promo_Week']].head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 53. Compare promo vs non-promo mean sales (by store type)\n",
        "\n",
        "**Q53.** For each store `Type`, compute mean `Weekly_Sales` for promo vs non-promo weeks.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "promo_type_mean = (\n",
        "    df.groupby(['Type', 'Is_Promo_Week'])['Weekly_Sales']\n",
        "      .mean()\n",
        "      .unstack('Is_Promo_Week')\n",
        "      .rename(columns={False: 'Non_Promo_Mean', True: 'Promo_Mean'})\n",
        ")\n",
        "\n",
        "promo_type_mean\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 54. Days since first record per store\n",
        "\n",
        "**Q54.** For each store, calculate `Days_Since_First_Date`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df = df.sort_values(['Store', 'Date'])\n",
        "first_date = df.groupby('Store')['Date'].transform('min')\n",
        "df['Days_Since_First_Date'] = (df['Date'] - first_date).dt.days\n",
        "df[['Store', 'Date', 'Days_Since_First_Date']].head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 55. Build a “feature matrix” for ML (store-dept weekly)\n",
        "\n",
        "**Q55.** Create a DataFrame with columns: `Store, Dept, Date, Weekly_Sales, Temperature, Fuel_Price, CPI, Unemployment, IsHoliday`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "feature_cols = ['Store', 'Dept', 'Date', 'Weekly_Sales',\n",
        "                'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'IsHoliday']\n",
        "\n",
        "feature_matrix = df[feature_cols].drop_duplicates()\n",
        "feature_matrix.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 56. One-hot encode store type and merge back\n",
        "\n",
        "**Q56.** Create dummy variables for `Type` and merge them into `feature_matrix`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "type_dummies = pd.get_dummies(df[['Store', 'Type']].drop_duplicates(), columns=['Type'])\n",
        "feature_matrix = feature_matrix.merge(type_dummies, on='Store', how='left')\n",
        "\n",
        "feature_matrix.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 57. Create lag features (1, 2, 3 weeks) for Weekly_Sales in feature_matrix\n",
        "\n",
        "**Q57.** For each `(Store, Dept)`, create `Lag_1`, `Lag_2`, `Lag_3` columns for `Weekly_Sales`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "feature_matrix = feature_matrix.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "group = feature_matrix.groupby(['Store', 'Dept'])['Weekly_Sales']\n",
        "feature_matrix['Lag_1'] = group.shift(1)\n",
        "feature_matrix['Lag_2'] = group.shift(2)\n",
        "feature_matrix['Lag_3'] = group.shift(3)\n",
        "\n",
        "feature_matrix.head(20)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 58. Train-test split by date for ML\n",
        "\n",
        "**Q58.** Split `feature_matrix` into **train** (dates before a cutoff) and **test** (dates on/after cutoff), e.g., cutoff = `'2012-01-01'`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "cutoff = pd.to_datetime('2012-01-01')\n",
        "\n",
        "train = feature_matrix[feature_matrix['Date'] < cutoff].copy()\n",
        "test  = feature_matrix[feature_matrix['Date'] >= cutoff].copy()\n",
        "\n",
        "train.shape, test.shape\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 59. Stratified sampling: 10% of rows per store for quick EDA\n",
        "\n",
        "**Q59.** Take a **10% sample** of rows from each store.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "sample_10_per_store = df.groupby('Store', group_keys=False).apply(\n",
        "    lambda g: g.sample(frac=0.1, random_state=42)\n",
        ")\n",
        "sample_10_per_store.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 60. Compute Gini-like inequality of sales per store\n",
        "\n",
        "**Q60.** For each store, compute a simple **Gini approximation**: sort weekly sales and use cumulative contributions.\n",
        "\n",
        "(Here we just compute cumulative sales; you can visualize for inequality.)\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "def store_lorenz(g):\n",
        "    s = g['Weekly_Sales'].sort_values()\n",
        "    cum = s.cumsum() / s.sum()\n",
        "    return cum.reset_index(drop=True)\n",
        "\n",
        "lorenz_example = store_lorenz(df[df['Store'] == 1])\n",
        "lorenz_example.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 61. Create month start and month end flags\n",
        "\n",
        "**Q61.** Add boolean columns: `Is_Month_Start`, `Is_Month_End` based on Date.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df['Is_Month_Start'] = df['Date'].dt.is_month_start\n",
        "df['Is_Month_End'] = df['Date'].dt.is_month_end\n",
        "df[['Date', 'Is_Month_Start', 'Is_Month_End']].head(10)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 62. Month-end average sales per store\n",
        "\n",
        "**Q62.** For each store, compute average sales on **month-end weeks** only.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "month_end_mean = (\n",
        "    df[df['Is_Month_End']]\n",
        "    .groupby('Store')['Weekly_Sales']\n",
        "    .mean()\n",
        ")\n",
        "\n",
        "month_end_mean.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 63. Compare size-normalized sales across store types\n",
        "\n",
        "**Q63.** Add `Sales_per_SqFt` and compute its mean for each `Type`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df['Sales_per_SqFt'] = df['Weekly_Sales'] / df['Size']\n",
        "df.groupby('Type')['Sales_per_SqFt'].mean()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 64. Create a “relative price index” for Fuel_Price and CPI\n",
        "\n",
        "**Q64.** Normalize `Fuel_Price` and `CPI` to 0–1 range and add columns `Fuel_Price_Norm`, `CPI_Norm`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "for col in ['Fuel_Price', 'CPI']:\n",
        "    col_min = df[col].min()\n",
        "    col_max = df[col].max()\n",
        "    df[f'{col}_Norm'] = (df[col] - col_min) / (col_max - col_min)\n",
        "\n",
        "df[['Fuel_Price', 'Fuel_Price_Norm', 'CPI', 'CPI_Norm']].head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 65. Calculate a simple composite “environmental index”\n",
        "\n",
        "**Q65.** Create `Env_Index = 0.5*CPI_Norm + 0.5*Unemployment_Norm` and compute its correlation with sales.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "u_min, u_max = df['Unemployment'].min(), df['Unemployment'].max()\n",
        "df['Unemployment_Norm'] = (df['Unemployment'] - u_min) / (u_max - u_min)\n",
        "\n",
        "df['Env_Index'] = 0.5*df['CPI_Norm'] + 0.5*df['Unemployment_Norm']\n",
        "\n",
        "df[['Weekly_Sales', 'Env_Index']].corr()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 66. Rolling mean of Env_Index vs sales correlation per store\n",
        "\n",
        "**Q66.** For each store, compute 20-week rolling correlation between `Env_Index` and `Weekly_Sales`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df = df.sort_values(['Store', 'Date'])\n",
        "\n",
        "def roll_corr_env_sales(g):\n",
        "    return (\n",
        "        g[['Weekly_Sales', 'Env_Index']]\n",
        "        .rolling(20, min_periods=10)\n",
        "        .corr()\n",
        "        .reset_index(level=0, drop=True)\n",
        "        .loc[:, ('Weekly_Sales', 'Env_Index')]\n",
        "    )\n",
        "\n",
        "df['Roll20_Corr_Env_Sales'] = (\n",
        "    df.groupby('Store', group_keys=False).apply(roll_corr_env_sales)\n",
        ")\n",
        "\n",
        "df[['Store', 'Date', 'Roll20_Corr_Env_Sales']].head(30)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 67. Identify stores most sensitive to unemployment\n",
        "\n",
        "**Q67.** Compute correlation between `Unemployment` and `Weekly_Sales` per store and list top 5 (by absolute value).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "unemp_corr = df.groupby('Store').apply(\n",
        "    lambda g: g['Weekly_Sales'].corr(g['Unemployment'])\n",
        ").rename('Unemp_Sales_Corr')\n",
        "\n",
        "unemp_corr.abs().sort_values(ascending=False).head(5)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 68. Use `assign` for a chained transformation\n",
        "\n",
        "**Q68.** In a single chained expression, select store 1 data, sorted by date, and compute `Lag_1`, `Lag_2`, `Sales_Change` = current − Lag_1.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "store1_chain = (\n",
        "    df[df['Store'] == 1]\n",
        "    .sort_values('Date')\n",
        "    .assign(\n",
        "        Lag_1=lambda x: x['Weekly_Sales'].shift(1),\n",
        "        Lag_2=lambda x: x['Weekly_Sales'].shift(2),\n",
        "        Sales_Change=lambda x: x['Weekly_Sales'] - x['Lag_1']\n",
        "    )\n",
        ")\n",
        "\n",
        "store1_chain.head(15)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 69. Grouped cumulative percentage of annual sales\n",
        "\n",
        "**Q69.** For each `(Store, Year)`, sort by Date and compute **cumulative percentage** of that year’s sales.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df = df.sort_values(['Store', 'Year', 'Date'])\n",
        "\n",
        "year_store_sales = df.groupby(['Store', 'Year'])['Weekly_Sales'].transform('sum')\n",
        "cum_sales = df.groupby(['Store', 'Year'])['Weekly_Sales'].cumsum()\n",
        "\n",
        "df['Cum_Sales_Pct_in_Year'] = cum_sales / year_store_sales * 100\n",
        "df[['Store', 'Year', 'Date', 'Weekly_Sales', 'Cum_Sales_Pct_in_Year']].head(20)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 70. Find the date when each store crosses 50% of its annual sales\n",
        "\n",
        "**Q70.** For each `(Store, Year)`, find the **first date** where `Cum_Sales_Pct_in_Year` ≥ 50.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "halfway = (\n",
        "    df[df['Cum_Sales_Pct_in_Year'] >= 50]\n",
        "    .groupby(['Store', 'Year'])['Date']\n",
        "    .min()\n",
        "    .rename('Halfway_Date')\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "halfway.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 71. Use `merge_asof` to approximate mapping from date to rolling CPI\n",
        "\n",
        "**Q71.** Compute daily CPI median (chain-level) and then join it back to df using `merge_asof` on Date.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "daily_cpi = (\n",
        "    df.groupby('Date')['CPI']\n",
        "      .median()\n",
        "      .reset_index()\n",
        "      .sort_values('Date')\n",
        "      .rename(columns={'CPI': 'CPI_Daily_Median'})\n",
        ")\n",
        "\n",
        "df = df.sort_values('Date')\n",
        "df = pd.merge_asof(\n",
        "    df,\n",
        "    daily_cpi,\n",
        "    on='Date',\n",
        "    direction='backward'\n",
        ")\n",
        "\n",
        "df[['Date', 'CPI', 'CPI_Daily_Median']].head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 72. Compare store size quartiles vs Avg sales per dept count\n",
        "\n",
        "**Q72.** For each `Size` quartile, compute average number of departments (`Dept`) per store and average total sales per store.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "stores['Size_Q'] = pd.qcut(stores['Size'], 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
        "\n",
        "store_dept_counts = df.groupby('Store')['Dept'].nunique().rename('Dept_Count')\n",
        "store_sales_tot = df.groupby('Store')['Weekly_Sales'].sum().rename('Total_Sales')\n",
        "\n",
        "store_info = (\n",
        "    stores[['Store', 'Size_Q']]\n",
        "    .merge(store_dept_counts, on='Store')\n",
        "    .merge(store_sales_tot, on='Store')\n",
        ")\n",
        "\n",
        "size_q_summary = (\n",
        "    store_info.groupby('Size_Q')\n",
        "              .agg(Avg_Dept_Count=('Dept_Count', 'mean'),\n",
        "                   Avg_Total_Sales=('Total_Sales', 'mean'))\n",
        ")\n",
        "\n",
        "size_q_summary\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 73. Identify departments missing in some stores\n",
        "\n",
        "**Q73.** Find departments that appear in **all stores** vs departments that are missing in at least one store.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "dept_store = df.groupby('Dept')['Store'].nunique()\n",
        "num_stores = df['Store'].nunique()\n",
        "\n",
        "depts_all_stores = dept_store[dept_store == num_stores].index\n",
        "depts_missing_some = dept_store[dept_store < num_stores].index\n",
        "\n",
        "depts_all_stores, depts_missing_some[:10]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 74. Create a “store-dept coverage matrix” (Store rows, Dept columns)\n",
        "\n",
        "**Q74.** Build a matrix with rows = `Store`, columns = `Dept`, value = 1 if dept exists in store, else 0.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "coverage = (\n",
        "    df.drop_duplicates(['Store', 'Dept'])\n",
        "      .assign(val=1)\n",
        "      .pivot(index='Store', columns='Dept', values='val')\n",
        "      .fillna(0)\n",
        "      .astype(int)\n",
        ")\n",
        "\n",
        "coverage.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 75. Identify “specialty” stores with less than N departments\n",
        "\n",
        "**Q75.** Find stores that have fewer than 10 departments.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "dept_count_per_store = df.groupby('Store')['Dept'].nunique()\n",
        "specialty_stores = dept_count_per_store[dept_count_per_store < 10]\n",
        "specialty_stores\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 76. Multi-step transformation: from daily CPI to monthly CPI index\n",
        "\n",
        "**Q76.** Compute monthly mean CPI (chain-level) and merge it to df as `CPI_Monthly_Mean`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "monthly_cpi = (\n",
        "    df.set_index('Date')\n",
        "      .resample('M')['CPI']\n",
        "      .mean()\n",
        "      .reset_index()\n",
        "      .rename(columns={'CPI': 'CPI_Monthly_Mean'})\n",
        ")\n",
        "\n",
        "df = df.merge(monthly_cpi, on='Date', how='left')\n",
        "df[['Date', 'CPI', 'CPI_Monthly_Mean']].head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 77. Use `transform` with custom function returning multiple columns\n",
        "\n",
        "**Q77.** For each store, compute **z-score** of `Weekly_Sales` and `Temperature` in a single groupby-transform step.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "def zscores_two_cols(g):\n",
        "    return pd.DataFrame({\n",
        "        'Sales_Z': (g['Weekly_Sales'] - g['Weekly_Sales'].mean()) / g['Weekly_Sales'].std(),\n",
        "        'Temp_Z': (g['Temperature'] - g['Temperature'].mean()) / g['Temperature'].std()\n",
        "    }, index=g.index)\n",
        "\n",
        "z_df = df.groupby('Store', group_keys=False).apply(zscores_two_cols)\n",
        "\n",
        "df['Sales_Z'] = z_df['Sales_Z']\n",
        "df['Temp_Z'] = z_df['Temp_Z']\n",
        "\n",
        "df[['Store', 'Date', 'Weekly_Sales', 'Sales_Z', 'Temp_Z']].head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 78. Bootstrap sample for a single store\n",
        "\n",
        "**Q78.** For store 1, draw a **bootstrap sample** (with replacement) of 100 rows and compute mean weekly sales of that sample.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "store1 = df[df['Store'] == 1]\n",
        "boot_sample = store1.sample(n=100, replace=True, random_state=42)\n",
        "boot_sample['Weekly_Sales'].mean()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 79. Multiple bootstrap estimates for store 1\n",
        "\n",
        "**Q79.** Repeat the above bootstrap 30 times and store mean sales of each sample in a Series.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "means = []\n",
        "for i in range(30):\n",
        "    s = store1.sample(n=100, replace=True, random_state=42 + i)\n",
        "    means.append(s['Weekly_Sales'].mean())\n",
        "\n",
        "boot_means = pd.Series(means, name='Bootstrap_Mean_Sales')\n",
        "boot_means.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 80. Compare volatility across store types using weekly std\n",
        "\n",
        "**Q80.** For each store type, compute standard deviation of weekly **chain-level** sales (sum over stores per date for that type).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "type_date = (\n",
        "    df.groupby(['Type', 'Date'])['Weekly_Sales']\n",
        "      .sum()\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "type_vol = (\n",
        "    type_date.groupby('Type')['Weekly_Sales']\n",
        "             .std()\n",
        "             .rename('Weekly_Std')\n",
        ")\n",
        "\n",
        "type_vol\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 81. Find “calendar-aligned” year-over-year sales change per store\n",
        "\n",
        "**Q81.** For each store, align weekly sales of year N and year N+1 **by ISO week** and compute YoY change.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "iso = df['Date'].dt.isocalendar()\n",
        "df['ISO_Year'] = iso.year\n",
        "df['ISO_Week'] = iso.week\n",
        "\n",
        "store_iso = df.groupby(['Store', 'ISO_Year', 'ISO_Week'])['Weekly_Sales'].sum().reset_index()\n",
        "\n",
        "store_iso['Next_Year'] = store_iso['ISO_Year'] + 1\n",
        "\n",
        "merged_yoy = store_iso.merge(\n",
        "    store_iso,\n",
        "    left_on=['Store', 'Next_Year', 'ISO_Week'],\n",
        "    right_on=['Store', 'ISO_Year', 'ISO_Week'],\n",
        "    suffixes=('_This', '_Next'),\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "merged_yoy['YoY_Change_%'] = (\n",
        "    (merged_yoy['Weekly_Sales_Next'] - merged_yoy['Weekly_Sales_This']) /\n",
        "     merged_yoy['Weekly_Sales_This'] * 100\n",
        ")\n",
        "\n",
        "merged_yoy.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 82. Identify departments with consistent positive YoY growth\n",
        "\n",
        "**Q82.** For each dept, compute annual total sales and check how many year gaps have positive YoY growth; list departments where all available YoY values are positive.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "dept_year = (\n",
        "    df.groupby(['Dept', 'Year'])['Weekly_Sales']\n",
        "      .sum()\n",
        "      .reset_index()\n",
        "      .sort_values(['Dept', 'Year'])\n",
        ")\n",
        "\n",
        "dept_year['YoY_Change'] = (\n",
        "    dept_year.groupby('Dept')['Weekly_Sales'].pct_change()\n",
        ")\n",
        "\n",
        "positive_only = (\n",
        "    dept_year.dropna()\n",
        "             .groupby('Dept')['YoY_Change']\n",
        "             .apply(lambda s: (s > 0).all())\n",
        ")\n",
        "\n",
        "depts_consistent_growth = positive_only[positive_only].index\n",
        "depts_consistent_growth\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 83. Add a feature: days to next holiday week per store\n",
        "\n",
        "**Q83.** For each store-date, compute `Days_To_Next_Holiday` (difference to nearest future holiday week for that store).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df = df.sort_values(['Store', 'Date'])\n",
        "\n",
        "def days_to_next_holiday(g):\n",
        "    holiday_dates = g.loc[g['IsHoliday'], 'Date']\n",
        "    next_holiday = holiday_dates.searchsorted(g['Date'], side='left')\n",
        "    # map index to actual future holiday date if exists\n",
        "    next_dates = []\n",
        "    for idx, nh in enumerate(next_holiday):\n",
        "        if nh < len(holiday_dates):\n",
        "            next_dates.append((holiday_dates.iloc[nh] - g['Date'].iloc[idx]).days)\n",
        "        else:\n",
        "            next_dates.append(pd.NA)\n",
        "    return pd.Series(next_dates, index=g.index)\n",
        "\n",
        "df['Days_To_Next_Holiday'] = df.groupby('Store', group_keys=False).apply(days_to_next_holiday)\n",
        "df[['Store', 'Date', 'IsHoliday', 'Days_To_Next_Holiday']].head(30)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 84. Fill NaNs in Days_To_Next_Holiday with a large number\n",
        "\n",
        "**Q84.** Replace NaNs in `Days_To_Next_Holiday` with 365.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df['Days_To_Next_Holiday'] = df['Days_To_Next_Holiday'].fillna(365)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 85. Compute average “Days_To_Next_Holiday” by store type\n",
        "\n",
        "**Q85.** For each `Type`, compute mean and median `Days_To_Next_Holiday`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df.groupby('Type')['Days_To_Next_Holiday'].agg(['mean', 'median'])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 86. Use `cut` to categorize Weekly_Sales into bins\n",
        "\n",
        "**Q86.** Create `Sales_Bin` with bins: [very low, low, medium, high] using 4 equal-width bins.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df['Sales_Bin'] = pd.cut(\n",
        "    df['Weekly_Sales'],\n",
        "    bins=4,\n",
        "    labels=['Very Low', 'Low', 'Medium', 'High']\n",
        ")\n",
        "\n",
        "df[['Weekly_Sales', 'Sales_Bin']].head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 87. Crosstab of Sales_Bin vs Type\n",
        "\n",
        "**Q87.** Show frequency table of `Sales_Bin` by `Type`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "pd.crosstab(df['Sales_Bin'], df['Type'])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 88. Weighted mean CPI per store using Weekly_Sales as weights\n",
        "\n",
        "**Q88.** For each store, compute **sales-weighted** average CPI.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "def weighted_mean_cpi(g):\n",
        "    return (g['CPI'] * g['Weekly_Sales']).sum() / g['Weekly_Sales'].sum()\n",
        "\n",
        "w_cpi = df.groupby('Store').apply(weighted_mean_cpi).rename('Weighted_CPI')\n",
        "w_cpi.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 89. Identify stores where weighted CPI > simple mean CPI\n",
        "\n",
        "**Q89.** Compare weighted CPI with simple mean CPI for each store and list stores where weighted CPI is higher.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "simple_cpi = df.groupby('Store')['CPI'].mean().rename('Mean_CPI')\n",
        "\n",
        "cpi_compare = pd.concat([w_cpi, simple_cpi], axis=1)\n",
        "cpi_compare['Weighted_GT_Mean'] = cpi_compare['Weighted_CPI'] > cpi_compare['Mean_CPI']\n",
        "\n",
        "cpi_compare[cpi_compare['Weighted_GT_Mean']].head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 90. Use `idxmin` / `idxmax` on a multi-index\n",
        "\n",
        "**Q90.** Build a Series of annual total sales per `(Store, Year)` and find the store-year with **lowest** and **highest** sales.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "store_year_sales = df.groupby(['Store', 'Year'])['Weekly_Sales'].sum()\n",
        "\n",
        "min_pair = store_year_sales.idxmin(), store_year_sales.min()\n",
        "max_pair = store_year_sales.idxmax(), store_year_sales.max()\n",
        "\n",
        "min_pair, max_pair\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 91. Rolling 4-week sales volatility per store-type\n",
        "\n",
        "**Q91.** For each `(Type, Store)` compute 4-week rolling std of `Weekly_Sales`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df = df.sort_values(['Type', 'Store', 'Date'])\n",
        "\n",
        "df['Roll4_Std'] = (\n",
        "    df.groupby(['Type', 'Store'])['Weekly_Sales']\n",
        "      .transform(lambda s: s.rolling(4, min_periods=2).std())\n",
        ")\n",
        "\n",
        "df[['Type', 'Store', 'Date', 'Weekly_Sales', 'Roll4_Std']].head(20)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 92. Identify “calm” stores: low volatility but high mean\n",
        "\n",
        "**Q92.** For each store, compute mean and std of `Weekly_Sales`. Flag stores with above-median mean and below-median std.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "store_stats = df.groupby('Store')['Weekly_Sales'].agg(['mean', 'std'])\n",
        "mean_med = store_stats['mean'].median()\n",
        "std_med = store_stats['std'].median()\n",
        "\n",
        "calm_stores = store_stats[\n",
        "    (store_stats['mean'] > mean_med) &\n",
        "    (store_stats['std'] < std_med)\n",
        "]\n",
        "\n",
        "calm_stores.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 93. Create an index per store relative to year 1\n",
        "\n",
        "**Q93.** For each store, pick its **first year** as base (index=100) and compute an index of annual sales for subsequent years.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "sy = df.groupby(['Store', 'Year'])['Weekly_Sales'].sum().rename('Year_Sales').reset_index()\n",
        "\n",
        "first_year_sales = sy.sort_values('Year').groupby('Store').first()['Year_Sales']\n",
        "first_year_sales.name = 'Base_Sales'\n",
        "\n",
        "sy = sy.merge(first_year_sales, on='Store', how='left')\n",
        "sy['Sales_Index'] = sy['Year_Sales'] / sy['Base_Sales'] * 100\n",
        "\n",
        "sy.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 94. Use `pivot_table` with custom lambda agg\n",
        "\n",
        "**Q94.** Create a pivot table of `Dept` (rows) × `Type` (cols) with values = **median** sales only on non-holiday weeks.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "non_holiday = df[~df['IsHoliday']]\n",
        "\n",
        "dept_type_median = pd.pivot_table(\n",
        "    non_holiday,\n",
        "    index='Dept',\n",
        "    columns='Type',\n",
        "    values='Weekly_Sales',\n",
        "    aggfunc=lambda s: s.median()\n",
        ")\n",
        "\n",
        "dept_type_median.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 95. Compare weekday vs weekend effect (if Date has weekday info)\n",
        "\n",
        "**Q95.** Add `Weekday` column and compute average sales by weekday (0=Mon,...,6=Sun).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "df['Weekday'] = df['Date'].dt.weekday\n",
        "weekday_mean = df.groupby('Weekday')['Weekly_Sales'].mean()\n",
        "weekday_mean\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 96. Groupby with `dropna=False` behavior\n",
        "\n",
        "**Q96.** Suppose some `MarkDown1` values are NaN. Group by `IsHoliday` and include NaN bucket as well to compute mean `MarkDown1`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "md1_group = df.groupby(['IsHoliday', pd.isna(df['MarkDown1'])], dropna=False)['MarkDown1'].mean()\n",
        "md1_group\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 97. Use `agg` with different functions per column\n",
        "\n",
        "**Q97.** For each store, compute:\n",
        "\n",
        "* `Weekly_Sales` → sum, mean\n",
        "* `Temperature` → mean\n",
        "* `Size` → first\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "store_multi_agg = df.groupby('Store').agg(\n",
        "    Total_Sales=('Weekly_Sales', 'sum'),\n",
        "    Avg_Sales=('Weekly_Sales', 'mean'),\n",
        "    Avg_Temp=('Temperature', 'mean'),\n",
        "    Size=('Size', 'first')\n",
        ")\n",
        "\n",
        "store_multi_agg.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 98. Filter groups using `filter`\n",
        "\n",
        "**Q98.** Keep only departments that have **at least 100 rows** in the dataset.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "filtered_df = df.groupby('Dept').filter(lambda g: len(g) >= 100)\n",
        "filtered_df['Dept'].nunique()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 99. Filter groups where holiday sales are higher than non-holiday sales\n",
        "\n",
        "**Q99.** Keep only those `(Store, Dept)` where mean holiday sales > mean non-holiday sales.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "def holiday_beats_nonholiday(g):\n",
        "    if g['IsHoliday'].any() and (~g['IsHoliday']).any():\n",
        "        holiday_mean = g[g['IsHoliday']]['Weekly_Sales'].mean()\n",
        "        nonholiday_mean = g[~g['IsHoliday']]['Weekly_Sales'].mean()\n",
        "        return holiday_mean > nonholiday_mean\n",
        "    return False\n",
        "\n",
        "hd_filtered = df.groupby(['Store', 'Dept']).filter(holiday_beats_nonholiday)\n",
        "hd_filtered[['Store', 'Dept']].drop_duplicates().head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 100. Create a “summary cube” with three dimensions\n",
        "\n",
        "**Q100.** Build a pivot table with index = `Type`, columns = `Season` (use your `Season` column or recreate), and values = total `Weekly_Sales`.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "def season_from_month(m):\n",
        "    if m in [12, 1, 2]:\n",
        "        return 'Winter'\n",
        "    elif m in [3, 4, 5]:\n",
        "        return 'Spring'\n",
        "    elif m in [6, 7, 8]:\n",
        "        return 'Summer'\n",
        "    else:\n",
        "        return 'Fall'\n",
        "\n",
        "df['Season'] = df['Month'].apply(season_from_month)\n",
        "\n",
        "season_cube = pd.pivot_table(\n",
        "    df,\n",
        "    index='Type',\n",
        "    columns='Season',\n",
        "    values='Weekly_Sales',\n",
        "    aggfunc='sum',\n",
        "    margins=True\n",
        ")\n",
        "\n",
        "season_cube\n",
        "```\n"
      ],
      "metadata": {
        "id": "_1LqVVQIAlaE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZxMjs4GlA18Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFYU_eFAAj2n"
      },
      "outputs": [],
      "source": []
    }
  ]
}